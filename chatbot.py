import os
import io
import time
import docx
import pandas as pd
from openai import OpenAI
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from google.oauth2 import service_account
import json

# 拽 专  专 专转 拽爪
class DriveConnector:
    def __init__(self):
        credentials_json = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS_JSON")
        if not credentials_json:
            raise ValueError("GOOGLE_APPLICATION_CREDENTIALS_JSON not set")
        credentials = service_account.Credentials.from_service_account_info(
            json.loads(credentials_json),
            scopes=["https://www.googleapis.com/auth/drive"]
        )
        self.drive_service = build('drive', 'v3', credentials=credentials)

    def get_file_content_by_name(self, file_name):
        results = self.drive_service.files().list(q=f"name='{file_name}'", fields="files(id)").execute()
        files = results.get('files', [])
        if not files:
            print(f" 爪 拽抓 砖: {file_name}")
            return None
        file_id = files[0]['id']
        request = self.drive_service.files().get_media(fileId=file_id)
        file_content = io.BytesIO()
        downloader = MediaIoBaseDownload(file_content, request)
        done = False
        while not done:
            _, done = downloader.next_chunk()
        return file_content.getvalue() if file_name.endswith(('.docx', '.xlsx')) else file_content.getvalue().decode('utf-8')

# 拽转 爪'
class ChatBot:
    def __init__(self, knowledge_files, api_key_file):
        self.drive_connector = DriveConnector()
        self.api_key = os.environ.get("OPENAI_API_KEY")
        self.client = OpenAI(api_key=self.api_key)
        self.embeddings = OpenAIEmbeddings(api_key=self.api_key)
        self.memory = ConversationBufferMemory()
        self.knowledge_files = knowledge_files if isinstance(knowledge_files, list) else [knowledge_files]
        self.vectorstore = None

    def load_knowledge_base(self):
        all_chunks = []
        for file_name in self.knowledge_files:
            if file_name.endswith('.docx'):
                chunks = self.process_docx_file(file_name)
                print(f"拽抓 Word {file_name} 注 注 {len(chunks)} 爪'拽.")
            elif file_name.endswith('.xlsx'):
                chunks = self.process_excel_file(file_name)
                print(f"拽抓 Excel {file_name} 注 注 {len(chunks)} 爪'拽.")
            else:
                content = self.drive_connector.get_file_content_by_name(file_name)
                if not content:
                    print(f"拽抓 {file_name}  爪   注 专.")
                    continue
                chunks = self.split_to_chunks(content)
                print(f"拽抓 拽住 {file_name} 注 注 {len(chunks)} 爪'拽.")
            all_chunks.extend(chunks)
        if not all_chunks:
            raise ValueError(" 爪 注 祝 拽抓 住住 注.")
        self.vectorstore = FAISS.from_texts(all_chunks, self.embeddings)
        print(f"住住 注 注 爪 注 住状 {len(all_chunks)} 爪'拽.")

    def process_docx_file(self, file_name):
        file_bytes = io.BytesIO(self.drive_connector.get_file_content_by_name(file_name))
        doc = docx.Document(file_bytes)
        text_content = "\n\n".join([para.text for para in doc.paragraphs if para.text.strip()])
        return self.split_to_chunks(text_content)

    def process_excel_file(self, file_name):
        file_bytes = io.BytesIO(self.drive_connector.get_file_content_by_name(file_name))
        excel_file = pd.ExcelFile(file_bytes)
        all_texts = []
        for sheet_name in excel_file.sheet_names:
            df = pd.read_excel(excel_file, sheet_name=sheet_name)
            headers = df.columns.tolist()
            for idx, row in df.iterrows():
                row_text = f": {sheet_name}\n"
                for col in headers:
                    if pd.notna(row[col]) and str(row[col]).strip():
                        row_text += f"{col}: {row[col]}\n"
                if len(row_text) > 50:
                    all_texts.append(row_text)
        chunks = []
        for text in all_texts:
            chunks.extend(self.split_to_chunks(text) if len(text) > 1500 else [text])
        return chunks

    def split_to_chunks(self, text, max_chunk_size=1500, overlap=100):
        sentences = text.split('. ')
        chunks = []
        current_chunk = ""
        for sentence in sentences:
            if len(current_chunk + sentence) <= max_chunk_size:
                current_chunk += sentence + '. '
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + '. '
        if current_chunk:
            chunks.append(current_chunk.strip())
        return chunks

    def get_relevant_chunks(self, question, k=3):
        docs = self.vectorstore.similarity_search(question, k=k)
        return [doc.page_content for doc in docs]

    def ask(self, question):
        context = '\n\n---\n\n'.join(self.get_relevant_chunks(question))
        history = self.memory.load_memory_variables({}).get('history', '')
        prompt = f"""
        转 注专 拽爪注, 拽 专专, 住注 爪 砖专转 拽转 拽转 注住拽 转专 专.
        转砖 爪专 转 驻专转, 专专, 拽转 注 住 注 爪 .
         转砖  拽转 注, 爪 转 驻专砖转. 转 转砖转 砖 拽住 3 砖专转

        住专转 砖 拽转:
        {history}

        注 专 砖:
        {context}

        砖:
        {question}
        """
        response = self.client.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=1000
        )
        answer = response.choices[0].message.content.strip()
        self.memory.save_context({"input": question}, {"output": answer})
        return answer

    def chat(self):
        print("爪' 驻注  砖转. 专砖 '爪'  住 转 砖.")
        while True:
            question = input(" 砖 砖: ")
            if question.lower() == '爪':
                print("爪' 住转, 转 转专转!")
                break
            start_time = time.time()
            answer = self.ask(question)
            elapsed_time = time.time() - start_time
            print(f"\n 转砖: {answer}\n憋  转: {elapsed_time:.2f} 砖转\n")

# 专爪 拽转  ( 转转 main.py  转注 )
if __name__ == "__main__":
    knowledge_files = ["住专 注 转 爪专驻转 3.txt"]
    bot = ChatBot(knowledge_files, None)
    bot.load_knowledge_base()
    bot.chat()
